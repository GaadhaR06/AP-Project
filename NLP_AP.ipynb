{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309f3e9c-9a3e-4fdb-97e5-8e76edfaa3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is the NLP part of Analytics Project\n",
    "#First "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee36cfe-3841-4800-adcd-ada32fdba209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-python\n",
      "  Using cached googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.12.14)\n",
      "Using cached googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googlesearch-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088620b6-a7e1-4853-83a7-8cd8dfcd3b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0adf47-3207-4abb-9773-387858b7a3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e5bcbf-4314-42be-aab3-929af894169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sanchita\\anaconda3\\envs\\nlp_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d72339b5-4c03-4c74-b291-e27a49cb0ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.decoder.version', 'model.encoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             GNAME           Classification\n",
      "0                         Achik Matgrik Army (AMA)  No information on Group\n",
      "1                A'chik Matgrik Elite Force (AMEF)  No information on Group\n",
      "2           Achik National Cooperative Army (ANCA)  No information on Group\n",
      "3            Achik National Liberation Army (ANLA)  No information on Group\n",
      "4          Achik National Volunteer Council (ANVC)     Separatist Terrorist\n",
      "..                                             ...                      ...\n",
      "312  Volunteers of Innocent People of Nagas (VIPN)  No information on Group\n",
      "313              Yimchunger Liberation Front (YLF)      Non terrorist Group\n",
      "314                                         Youths      Non terrorist Group\n",
      "315                       Zeliangrong United Front  No information on Group\n",
      "316                  Zomi Revolutionary Army (ZRA)      Non terrorist Group\n",
      "\n",
      "[317 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from transformers import pipeline \n",
    "import pandas as pd\n",
    "\n",
    "names_sheet=pd.read_excel('GroupName_terrorists.xlsx')\n",
    "names_sheet.dropna()\n",
    "df=pd.DataFrame(names_sheet)\n",
    "\n",
    "########################################################################################\n",
    "# Initialize the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_first_google_link(gname):\n",
    "    for result in search(gname, num_results=10):\n",
    "        if 'wikipedia.org' in result:\n",
    "            return result\n",
    "        elif 'satp.org' in result:\n",
    "            return result\n",
    "    return None\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "def get_textual_content(group_name):\n",
    "    # Get the first two sentences from the Wikipedia page\n",
    "    first_link = get_first_google_link(group_name)\n",
    "    if first_link:\n",
    "        url = first_link\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = soup.find_all('p') \n",
    "            \n",
    "            # Extract text from the paragraphs\n",
    "            text = ' '.join([para.get_text() for para in paragraphs]) #getting full content of wikipedia\n",
    "            \n",
    "            # Split the text into sentences\n",
    "            sentences = text.split('. ')\n",
    "            \n",
    "            # Return the first four sentences\n",
    "            return '. '.join(sentences[:4]) + '.' if len(sentences) > 1 else None #if len(text)=1 it means text=name of grp\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "def classify_group(text):\n",
    "    \n",
    "    classes_labels = [\"Religious Terrorist Group\", \"Separatist Terrorist\", \"Left-Wing Terrorist Group\", \"Right-Wing Terrorist Group\", \"Non terrorist Group\"]\n",
    "    result = classifier(text, classes_labels)\n",
    "    return (result['labels'][0])\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "group_names = df['GNAME'].to_list()  \n",
    "classified_list=[]\n",
    "for group in group_names:\n",
    "    content = get_textual_content(group)\n",
    "    if content!= None:\n",
    "        classified_list.append(classify_group(content))\n",
    "    else: \n",
    "        classified_list.append('No information on Group')\n",
    "\n",
    "\n",
    "length=len(df['GNAME'])-len(classified_list)\n",
    "\n",
    "for i in range(length):\n",
    "    classified_list.append('dummy')\n",
    "\n",
    "df['Classification']=classified_list\n",
    "print(df[['GNAME', 'Classification']])\n",
    "\n",
    "df.to_excel('output_file.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760ace1f-7632-48f4-a473-636d6193a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.decoder.version', 'model.encoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'I have earned huge returns by investing into bitcoin', 'labels': ['Finance', 'Technology', 'Agriculture', 'Oil and Gas'], 'scores': [0.6344659924507141, 0.3104734718799591, 0.03644862398505211, 0.018611909821629524]}\n"
     ]
    }
   ],
   "source": [
    "    from transformers import pipeline \n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "    classes_labels = [\"Finance\",\"Oil and Gas\", \"Technology\", \"Agriculture\"]\n",
    "    result = classifier(\"I have earned huge returns by investing into bitcoin\", classes_labels)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b2041-5884-4bdb-9cbd-29a2bf121895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28ffb5-56e9-4fdd-afcc-6b1c2ce8960c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
